{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex06 Red Neuronal Convolucional en TensorFlow\n",
    "\n",
    "En esta actividad, implementaremos una red neuronal convolucional utilizando TensorFlow para abordar un problema de clasificaci칩n. Se asume que ya estas familiarizado con conceptos b치sicos de TensorFlow como el uso de constantes, variables, tipos de datos, sesiones, placeholders y grafos de c칩mputo.\n",
    "\n",
    "Aprendizaje esperados:\n",
    "\n",
    "- Implementar algunas funciones de utiler칤a que utilizaremos en el modelo ConvNet\n",
    "- Construir y entrenar una ConvNet en TensorFlow para un problema de clasificaci칩n\n",
    "\n",
    "## Uso de librer칤as\n",
    "\n",
    "Para comenzar con la actividad, importemos las librer칤as que utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploraci칩n del dataset de entrenamiento\n",
    "\n",
    "En esta actividad, utilizaremos un conjunto de datos que contienen im치genes con se침as que representan n칰meros del 0 al 5. El conjunto de datos se encuentra organizado en dos archivos: `train_signs.h5` que utilizaremos para entrenamiento y `test_signs.h5` que utilizaremso para realizar las pruebas. Ejecutemos la siguiente celda para cargar los datasets que vamos a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos (signs)\n",
    "# utilicemos la funci칩n load_dataset() definida en utils.py\n",
    "\n",
    "X_train_original, Y_train_original, X_test_original, Y_test_original, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SIGNS.png\" style=\"width:600px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos una imagen del conjunto de datos de entrenamiento\n",
    "i = 7\n",
    "plt.imshow(X_train_original[i])\n",
    "print (f\"y = {Y_train_original[:, i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos visto el tipo de im치genes que contienen los `dataset`, examinemos las dimensiones de los conjuntos de datos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"N칰mero de ejemplos de entrenamiento: {None}\")\n",
    "print (f\"N칰mero de ejemplos de prueba:  {None}\")\n",
    "print (f\"Dimensiones de X_train_original: {None}\")\n",
    "print (f\"Dimensiones de Y_train_original: {None}\")\n",
    "print (f\"Dimensiones de X_test_original shape: {None}\")\n",
    "print (f\"Dimensiones de Y_test_original: {None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar, normalicemos las im치genes y realicemos la conversi칩n de las etiquetas a una representaci칩n One hot (para esto podemos utilizar la funci칩n one_hot definida en `utils.py`). Verifiquemos las dimensiones de `Y_train` y `Y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None     #Normalicemos las im치genes de entrenamiento\n",
    "X_test = None      #Normalicemos las im치genes de pruebas\n",
    "Y_train = None     #one_hot(dataset, n칰mero de clases).T\n",
    "Y_test = None      #one_hot(dataset, n칰mero de clases).T\n",
    "\n",
    "print (f\"Dimensiones de Y_train_original: {None}\")\n",
    "print (f\"Dimensiones de Y_test_original: {None}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelo en TensorFlow\n",
    "\n",
    "Antes de construir el modelo, implementemos algunas funciones que utilizaremos posteriormente en el modelo de nuestra ConvNet.\n",
    "\n",
    "- create_placeholder\n",
    "- init_parameters\n",
    "- forward_propagation\n",
    "- compute_cost\n",
    "\n",
    "### 2.1 Funciones base\n",
    "\n",
    "#### 2.1.1 Creaci칩n de los placeholders\n",
    "\n",
    "TensorFlow requiere que definamos `placeholders` para los datos de entrada que se introducir치n en el modelo cuando se ejecute la sesi칩n. Implementemos la funci칩n `create_placeholder` para crear `placeholders` para las im치genes de entrada $X$ y la salida esperada $Y$. No debemos definir el n칰mero de ejemplos de entrenamiento por el momento. Para hacerlo, puede usar la palabra `None` como el tama침o del lote. Esto le dar치 la flexibilidad para definirlo m치s tarde. As칤, $X$ debe ser de dimensiones: [None, n_H, n_W, n_C] y $Y$ debe ser de dimensiones [None, n_y].\n",
    "\n",
    "[M치s informaci칩n](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholder(n_H, n_W, n_C, n_y):\n",
    "    \"\"\"\n",
    "    Crea un placeholder para la sesi칩n de TensorFlow.\n",
    "    \n",
    "    Par치metros:\n",
    "    n_H -- alto de una imagen de entrada\n",
    "    n_W -- ancho de una imagen de entrada\n",
    "    n_C -- n칰mero de canales la imagen\n",
    "    n_y -- n칰mero de clases\n",
    "        \n",
    "    Retorna:\n",
    "    X -- placeholder para datos de entrada, de dimensiones [None, n_H, n_W, n_C] y dtype \"float\"\n",
    "    Y -- placeholder para etiquetas de entrada, de dimensiones [None, n_y] y dtype \"float\"\n",
    "    \"\"\"\n",
    "    ### INICIA TU C칍DIGO ### \n",
    "    X = None\n",
    "    Y = None\n",
    "    ### FINALIZA TU C칍DIGO ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el uso de nuestra funci칩n create_placeholder acorde a las dimensiones de las im치genes en los datasets\n",
    "X, Y = create_placeholder(64, 64, 3, 6)\n",
    "print (f\"X : {None}\")\n",
    "print (f\"Y : {None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Inicializaci칩n de par치metros\n",
    "\n",
    "Inicialicemos los filtros: $洧녥1$ y $W2$ utilizando la funci칩n `tf.contrib.layers.xavier_initializer (seed = 0)`. No necesita preocuparse por los `bias`, ya que las funciones de TensorFlow se ocupan del `bias`. Adicionalmente, considere que s칩lo inicializar치 los filtros para las funciones `conv2d` (TensorFlow inicializa las capas `FC` autom치ticamente). Implementemos la funci칩n `init_parameters()`. \n",
    "\n",
    "Para inicializar un par치metro 洧녥 de dimensiones [1, 2, 3, 4] en TensorFlow, utilice:\n",
    "\n",
    "```python\n",
    "W = tf.get_variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "\n",
    "[M치s informaci칩n](https://www.tensorflow.org/api_docs/python/tf/get_variable).\n",
    "\n",
    "Las dimensiones para cada grupo de filtros son:\n",
    "- W1 : [4, 4, 3, 8]\n",
    "- W2 : [2, 2, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    \"\"\"\n",
    "    Inicializar los pesos para construir la red neuornal con TensorFlow.\n",
    "    \n",
    "    Retorna:\n",
    "    parameters --un diccionario de tensores conteniendo W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                              # para que sus valores aleatorios concuerden\n",
    "        \n",
    "    ### INICIA TU C칍DIGO ### \n",
    "    W1 = tf.get_variable(\"W1\",[4,4,3,8],initializer = tf.contrib.layers.xavier_initializer(seed = 0) )\n",
    "    W2 = None\n",
    "    ### FIN DE TU C칍DIGO ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "test_session= tf.Session()\n",
    "parameters = init_parameters()\n",
    "init = tf.global_variables_initializer()\n",
    "test_session.run(init)\n",
    "print(f\"W1 = {parameters['W1'].eval(test_session)[1,1,1]}\")\n",
    "print(f\"W2 = {parameters['W2'].eval(test_session)[1,1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Forward propagation\n",
    "\n",
    "TensorFlow provee un conjuto de funciones para realizar las convoluciones (no es necesario implementarlas desde cero).\n",
    "\n",
    "- tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = 'SAME')\n",
    "\n",
    "    - Dada una entrada `X` y un grupo de filtros `W`, la funci칩n `conv2` aplica la convoluci칩n de `W` sobre `X`. El tercer par치metro `[1, s, s, 1]` representa el `stride` para cada dimensi칩n de la entrada `(m, n_H_prev, n_W_prev, n_C_prev)`. [[documentaci칩n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "\n",
    "- tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME')\n",
    "    - Data una entrada `A`, la funci칩n max_pool utiliza una ventana de dimensiones `(f, f)` y `stride` de tama침o `(s, s)` para realizar el `max pooling` sobre cada ventana. [[documentaci칩n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/max_pool)\n",
    "    \n",
    "    \n",
    "- tf.nn.relu(Z)\n",
    "    - Calcula la funci칩n `ReLU` a los elementos de `Z` (puede tener cualquier forma). [[documentaci칩n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/relu)\n",
    "\n",
    "\n",
    "- tf.contrib.layers.flatten(A)\n",
    "    - Dada una entrada A, esta funci칩n aplana cada ejemplo en un vector 1D mientras mantiene el tama침o del lote. Devuelve un tensor aplanado con forma `[batch_size, k]`. [[documentaci칩n]](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
    "\n",
    "\n",
    "- tf.contrib.layers.fully_connected(F, num_outputs)\n",
    "    - Dada una entrada aplanada F, devuelve la salida calculada usando una capa completamente conectada. [[documentaci칩n]](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
    "\n",
    "$\\textbf{Nota}$: La funci칩n `tf.contrib.layers.fully_connected`, inicializa autom치ticamente los pesos en el grafo y contin칰a entren치ndolos durante el entrenamiento del modelo. Por lo tanto, no necesita inicializar esos pesos al inicializar los par치metros.\n",
    "\n",
    "Utilizando las funciones descritas anteriormente, implementemos la funci칩n `forward_propagation`  para construir el siguiente modelo: \n",
    "\n",
    "\n",
    "`CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`\n",
    "\n",
    "\n",
    "Para este ejercicio, utilicemos los siguientes par치metros:\n",
    "\n",
    "- $\\textbf{CONV2D}$: stride=1, padding=\"SAME\"\n",
    "\n",
    "- $\\textbf{RELU}$\n",
    "\n",
    "- $\\textbf{MAXPOOL}$: filtro de 8x8, stride = 8, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{CONV2D}$: stride=1, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{RELU}$\n",
    "\n",
    "- $\\textbf{MAXPOOL}$: filtro de 4x4, stride = 4, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{FLATTEN}$\n",
    "\n",
    "- $\\textbf{FULLYCONNECTED (FC) layer}$.\n",
    "\n",
    "$\\text{Nota}$: la capa FC dar치 como resultado 6 neuronas en la capa de salida, que luego pasar치n a un funci칩n `softmax`. En TensorFlow, las funciones `softmax` y `cost` se agrupan en una sola funci칩n, que se invocar치 posteriormente al calcular el costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementaci칩n de forward propagation para el modelo:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Par치metros:\n",
    "    X             placeholder del dataset de entrada, de dimensiones (input size, number of examples)\n",
    "    parameters    diccionario de python que contiene los par치metros W1, W2\n",
    "                  recordemos que las dimensiones se definen en la inicializaci칩n de par치metros.\n",
    "\n",
    "    Retorna:\n",
    "    Z3            la salida de la 칰ltima unidad lineal\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recupera los par치metros del diccionario \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    ### INICIA TU C칍DIGO ###\n",
    "    \n",
    "    # CONV2D: filtros = W1, stride = 1, padding = 'SAME'\n",
    "    Z1 = None\n",
    "    \n",
    "    # RELU\n",
    "    A1 = None\n",
    "    \n",
    "    # MAXPOOL: filtro = 8x8, stride = 8, padding = 'SAME'\n",
    "    P1 = None\n",
    "    \n",
    "    # CONV2D: filtros = W2, stride=1, padding='SAME'\n",
    "    Z2 = None\n",
    "    \n",
    "    # RELU\n",
    "    A2 = None\n",
    "    \n",
    "    # MAXPOOL: filtro = 4x4, stride 4, padding 'SAME'\n",
    "    P2 = None\n",
    "    \n",
    "    # FLATTEN\n",
    "    P2 = None\n",
    "    \n",
    "    # Capa FC con \"activation_fn=None\" \n",
    "    Z3 = None\n",
    "    \n",
    "    ### FIN DE TU CODIGO ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session=tf.Session()\n",
    "np.random.seed(1)\n",
    "X, Y = create_placeholder(64, 64, 3, 6)\n",
    "parameters = init_parameters()\n",
    "Z3 = forward_propagation(X, parameters)\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "a = session.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "print(\"Z3 = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Calcular de costo\n",
    "\n",
    "Implementemos el c치lculo de la funci칩n de costo. Para esto, utilizaremos las siguiente funciones que proporciona TensorFlow y que nos permiten evitar una implementaci칩 desde cero.\n",
    " \n",
    "- tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y)\n",
    "    - Esta funci칩n calcula la funci칩n de activaci칩n softmax y la p칠rdida resultante.  [[Documentaci칩n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "\n",
    "\n",
    "- tf.reduce_mean\n",
    "    - Calcula la media de elementos a trav칠s de las dimensiones de un tensor. Utilice esta funci칩n para sumar las p칠rdidas sobre todos los ejemplos y as칤 obtener el costo total. [[Documentaci칩n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/math/reduce_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(Z, Y):\n",
    "    \"\"\"\n",
    "    Calcular el costo\n",
    "    \n",
    "    Par치metros:\n",
    "    Z     salida del forward propagation (salida de la 칰ltima unidad de activaci칩n lineal), de dimensiones (6, numero de ejemplos)\n",
    "    Y     placeholder de vector con etiquetas reales, tiene la misma dimensi칩n que Z\n",
    "    \n",
    "    Retorna:\n",
    "    cost Tensor del costo\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIO DE TU C칍DIGO ###\n",
    "    clogs = None\n",
    "    cost = None\n",
    "    ### FIN DE TU C칍DIGO ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholder(64, 64, 3, 6)\n",
    "    parameters = init_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = cost_function(Z3, Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    a = session.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Integraci칩n del modelo ConvNet\n",
    "\n",
    "Finalmente, integremos las funciones que implementamos anteriormente para construir un modelo. Posteriormente, entrenaremos el conjunto de datos `train_signs.h5` utilizando mini lotes. En el archivo `utils.py` se encuentra implementada la funci칩n `random_mini_batches()`. Recuerde que esta funci칩n devuelve una lista de mini lotes.\n",
    "\n",
    "El modelo deber칤a:\n",
    "\n",
    "- Crear placeholders\n",
    "- Inicializar par치metros\n",
    "- Realizar el forward propagation\n",
    "- Calcular el costo\n",
    "- Crear un optimizador\n",
    "\n",
    "Finalmente, crear치 una sesi칩n y ejecutar치 un bucle for para num_epochs, obtendr치 los mini lotes y luego, para cada mini lote, optimizar치 la funci칩n. [[doc para inicializaci칩n las variables]](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)\n",
    "\n",
    "Completemos la siguiente funci칩n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.006, num_epochs = 100, minibatch_size = 64):\n",
    "    \"\"\"\n",
    "    Recordemos la estructura de la ConvNet:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Par치metros: \n",
    "    X_train          dataset de entrenamiento, dimensiones (None, 64, 64, 3)\n",
    "    Y_train          etiquetas reales del dataset de entrenamiento, dimensiones (None, n_y = 6)\n",
    "    X_test           dataset de pruebas, dimensiones (None, 64, 64, 3)\n",
    "    Y_test           etiquetas reales del dataset de pruebas, dimensiones (None, n_y = 6)\n",
    "    learning_rate    tasa de aprendizaje del optimizador\n",
    "    num_epochs       n칰mero de epocas del ciclo del optimizador\n",
    "    minibatch_size   tama침o del mini batch \n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                   # permite ejecutar el modelo sin sobre escribir las variables tf\n",
    "    tf.set_random_seed(1)                       # para mantener los resultados consistentes tensorflow\n",
    "    seed = 3                                    # para mantener los resultados consistentes en numpy \n",
    "    \n",
    "    (m, n_H0, n_W0, n_C0) = None       # identificar las dimensiones del dataset de entrenamiento\n",
    "    n_y = None                      # identificar el n칰mero de etiquetas\n",
    "    costs = []                                  # Para mantener un registro de los costos\n",
    "    \n",
    "    # Crear placeholders con las dimensiones correctas\n",
    "    ### INICIA TU C칍DIGO ### \n",
    "    X, Y = None\n",
    "    ### FINALIZA TU C칍DIGO ###\n",
    "\n",
    "    # Inicializar los par치metros\n",
    "    ### INICIA TU C칍DIGO ### (1 line)\n",
    "    parameters = None\n",
    "    ### FINALIZA TU C칍DIGO ###\n",
    "    \n",
    "    # Forward propagation: crear el forward propagation en el grafo de c칩mputo de TensorFlow\n",
    "    ### INICIA TU C칍DIGO ### (1 line)\n",
    "    Z3 = None\n",
    "    ### FINALIZA TU C칍DIGO ###\n",
    "    \n",
    "    # Funci칩n de costo: agrega la funci칩n de costo al grafo de c칩mputo\n",
    "    ### INICIA TU C칍DIGO ### (1 line)\n",
    "    cost = None\n",
    "    ### FINALIZA TU C칍DIGO ###\n",
    "    \n",
    "    # Backpropagation: definir el optimizador en TensorFlow. Utilicemos un optimizador AdamOptimizer\n",
    "    # para minimizar el costo\n",
    "    \n",
    "    ### INICIA TU C칍DIGO ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    ### FINALIZA TU C칍DIGO ###\n",
    "    \n",
    "    # Inicializar globalmente todas las variables\n",
    "    init = None\n",
    "     \n",
    "    # Iniciar la sesi칩n para evaluar el grafo de TensorFlow\n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        # Ejecutar la inicializaci칩n\n",
    "        session.run(None)\n",
    "        \n",
    "        # Ciclo de entrenamiento\n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) #Definida en utils.py\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Selecciona un minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: Ejecuci칩n del grafo sobre un minibatch.\n",
    "                # Ejecutar la sesi칩n para ejecutar el optimizador y la funci칩n de costo, t\n",
    "                # el feed_dict debe contener un minibatch (X,Y).\n",
    "                ### INICIA TU C칍DIGO ### \n",
    "                _ , temp_cost = session.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                ### FINALIZA TU C칍DIGO ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "            # Imprimir el costo\n",
    "            if epoch % 10 == 0:\n",
    "                print (f\"Costo despues de la epoca {epoch}: {None}\")\n",
    "            if epoch % 1 == 0:\n",
    "                costs.append(None)\n",
    "        \n",
    "        \n",
    "        # graficar el costo\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('Costo')\n",
    "        plt.xlabel('Epocas')\n",
    "        plt.title(\"Tasa de aprendizaje:\" + str(None))\n",
    "        plt.show()\n",
    "\n",
    "        # Calcular las predicciones correctas\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calcular la exactitud sobre el conjunto de pruebas\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Exactitud (Training dataset):\", train_accuracy)\n",
    "        print(\"Exactitud (Test datset):\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
